import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def extract_from_html(html_url, base_dir):
    os.makedirs(base_dir, exist_ok=True)
    img_dir = os.path.join(base_dir, "images")
    os.makedirs(img_dir, exist_ok=True)

    # ä¸‹è¼‰ HTML
    r = requests.get(html_url)
    html = r.text
    with open(os.path.join(base_dir, "source.html"), "w", encoding="utf-8") as f:
        f.write(html)
    soup = BeautifulSoup(html, 'html.parser')

    md_lines = []

    # æ‰¾ä¸»è¦å…§å®¹å€åŸŸï¼Œé€šå¸¸åœ¨ <main>, <article> æˆ–æœ‰ç‰¹å®š class çš„ <div>
    main_content = soup.find('main') or soup.find('article') or soup.find('body')
    
    if main_content:
        # å°ˆæ³¨æ–¼ç« ç¯€æ¨™é¡Œ(h2)å’Œæ®µè½å…§å®¹(p)çš„è™•ç†
        processed_elements = set()  # é¿å…é‡è¤‡è™•ç†
        
        # é¦–å…ˆè™•ç†æ‰€æœ‰å…ƒç´ ï¼ŒæŒ‰æ–‡æª”é †åº
        all_elements = main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p'])
        
        for element in all_elements:
            # è·³éå·²è™•ç†çš„å…ƒç´ 
            if id(element) in processed_elements:
                continue
                
            # è·³éå°èˆªã€å´é‚Šæ¬„ç­‰éå…§å®¹å€åŸŸ
            classes = element.get('class', [])
            if any(cls in ['nav', 'sidebar', 'menu', 'footer', 'header', 'ad', 'advertisement', 'toc'] for cls in classes):
                continue
            
            tag_name = element.name
            
            # è™•ç†ç« ç¯€æ¨™é¡Œ (h2 ç‚ºä¸»è¦ç« ç¯€)
            if tag_name == 'h2':
                title_text = extract_text_with_math(element)
                if title_text.strip():
                    md_lines.append(f"## {title_text.strip()}")
                    md_lines.append("")  # æ¨™é¡Œå¾ŒåŠ ç©ºè¡Œ
                    processed_elements.add(id(element))
            
            # è™•ç†å…¶ä»–æ¨™é¡Œå±¤ç´š
            elif tag_name in ['h1', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag_name[1])
                title_text = extract_text_with_math(element)
                if title_text.strip():
                    md_lines.append(f"{'#' * level} {title_text.strip()}")
                    md_lines.append("")
                    processed_elements.add(id(element))
            
            # è™•ç†æ®µè½å…§å®¹ (p ç‚ºä¸»è¦æ–‡ç« å…§å®¹)
            elif tag_name == 'p':
                paragraph_content = extract_text_with_math(element)
                if paragraph_content.strip():
                    # æ¸…ç†æ®µè½å…§å®¹ï¼Œç§»é™¤å¤šé¤˜ç©ºç™½
                    clean_content = ' '.join(paragraph_content.split())
                    if len(clean_content) > 10:  # åªä¿ç•™æœ‰æ„ç¾©çš„æ®µè½
                        md_lines.append(clean_content)
                        md_lines.append("")  # æ®µè½å¾ŒåŠ ç©ºè¡Œ
                    processed_elements.add(id(element))
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°h2æ¨™é¡Œï¼Œå˜—è©¦æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ç« ç¯€æ¨™è­˜
        if not any('## ' in line for line in md_lines):
            print("âš ï¸ æœªæ‰¾åˆ° h2 æ¨™é¡Œï¼Œå˜—è©¦æŸ¥æ‰¾å…¶ä»–ç« ç¯€æ¨™è­˜...")
            
            # å°‹æ‰¾å¯èƒ½çš„ç« ç¯€æ¨™é¡Œ (å…·æœ‰ç‰¹å®šclassæˆ–idçš„div)
            section_divs = main_content.find_all(['div', 'section'], 
                class_=lambda x: x and any(keyword in str(x).lower() 
                for keyword in ['title', 'heading', 'section', 'chapter']))
            
            for div in section_divs:
                if id(div) not in processed_elements:
                    div_text = extract_text_with_math(div)
                    if div_text.strip() and len(div_text.strip()) < 100:  # å¯èƒ½æ˜¯æ¨™é¡Œ
                        md_lines.append(f"## {div_text.strip()}")
                        md_lines.append("")
                        processed_elements.add(id(div))

    # æ¸…ç†å’Œå„ªåŒ–Markdownå…§å®¹
    cleaned_md_lines = []
    for line in md_lines:
        if line.strip():  # åªä¿ç•™éç©ºè¡Œ
            cleaned_md_lines.append(line)
        elif cleaned_md_lines and cleaned_md_lines[-1] != "":
            # é¿å…é€£çºŒç©ºè¡Œï¼Œåªåœ¨éœ€è¦æ™‚åŠ å…¥å–®å€‹ç©ºè¡Œ
            cleaned_md_lines.append("")
    
    # å„²å­˜ç‚ºMarkdownæ ¼å¼
    with open(os.path.join(base_dir, "text.md"), "w", encoding="utf-8") as f:
        if cleaned_md_lines:
            f.write('\n'.join(cleaned_md_lines))
            # ç¢ºä¿æ–‡ä»¶ä»¥æ›è¡ŒçµæŸ
            if not cleaned_md_lines[-1] == "":
                f.write('\n')
        else:
            f.write("# ç„¡æ³•æå–å…§å®¹\n\næœªèƒ½å¾HTMLä¸­æå–åˆ°æœ‰æ•ˆçš„æ–‡å­—å…§å®¹ã€‚")
    
    print(f"âœ… Markdownæ–‡ä»¶å·²ä¿å­˜ï¼Œå…± {len([l for l in cleaned_md_lines if l.strip()])} è¡Œå…§å®¹")

    # ä¸‹è¼‰åœ–ç‰‡ï¼Œä»ç”¨ä½ çš„ç­–ç•¥
    img_count = 1
    if not html_url.endswith('/'):
        html_url += '/'
    for img in soup.find_all("img"):
        src = img.get("src")
        if not src:
            continue
        img_url = urljoin(html_url, src)
        try:
            r_img = requests.get(img_url, timeout=15)
            if r_img.status_code == 200:
                fname = f"img_{img_count}.png"
                with open(os.path.join(img_dir, fname), "wb") as fout:
                    fout.write(r_img.content)
                img_count += 1
            else:
                print(f"Failed to download image {img_url}: HTTP {r_img.status_code}")
        except Exception as e:
            print(f"Error downloading image {img_url}: {e}")

    print(f"HTML extraction complete. Images and text.md saved in: {base_dir}")


def extract_text_with_math(element):
    """å¾HTMLå…ƒç´ ä¸­æå–æ–‡å­—å’Œæ•¸å­¸å…¬å¼ï¼Œè½‰æ›ç‚ºMarkdownæ ¼å¼"""
    if not element:
        return ""
    
    result = ""
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«å­å…ƒç´ 
    if element.find():
        for child in element.children:
            if hasattr(child, 'name') and child.name:
                # è™•ç†æ•¸å­¸å…¬å¼
                if child.name in ['span', 'div']:
                    classes = child.get('class', [])
                    if any(c.lower() in ['math', 'katex', 'mjx', 'mathjax', 'mathml'] for c in classes):
                        latex = child.get_text().strip()
                        if latex:
                            if '\n' in latex or len(latex) > 40:
                                result += f" $$\n{latex}\n$$ "
                            else:
                                result += f" ${latex}$ "
                    else:
                        # éæ­¸è™•ç†å…¶ä»– span/div
                        nested_text = extract_text_with_math(child)
                        if nested_text.strip():
                            result += nested_text
                
                elif child.name == 'math':
                    latex = child.get_text().strip()
                    if latex:
                        result += f" $$\n{latex}\n$$ "
                
                # è™•ç†å¼·èª¿æ ¼å¼
                elif child.name in ['strong', 'b']:
                    text = child.get_text().strip()
                    if text:
                        result += f" **{text}** "
                elif child.name in ['em', 'i']:
                    text = child.get_text().strip()
                    if text:
                        result += f" *{text}* "
                elif child.name == 'code':
                    text = child.get_text().strip()
                    if text:
                        result += f" `{text}` "
                
                # è™•ç†é€£çµ
                elif child.name == 'a':
                    href = child.get('href', '')
                    text = child.get_text().strip()
                    if text:
                        if href and not href.startswith('#'):  # è·³ééŒ¨é»é€£çµ
                            result += f" [{text}]({href}) "
                        else:
                            result += f" {text} "
                
                # è™•ç†æ›è¡Œå’Œç©ºæ ¼
                elif child.name == 'br':
                    result += " \n"
                
                # è·³éä¸éœ€è¦çš„æ¨™ç±¤
                elif child.name in ['script', 'style', 'noscript']:
                    continue
                
                # å…¶ä»–æ¨™ç±¤éæ­¸è™•ç†
                else:
                    nested_text = extract_text_with_math(child)
                    if nested_text.strip():
                        result += nested_text
            else:
                # ç´”æ–‡å­—ç¯€é»
                text = str(child).strip()
                if text:
                    result += f" {text} "
    else:
        # æ²’æœ‰å­å…ƒç´ ï¼Œç›´æ¥å–æ–‡å­—
        text = element.get_text().strip()
        if text:
            result += f" {text} "
    
    # æ¸…ç†çµæœï¼šç§»é™¤å¤šé¤˜ç©ºç™½ï¼Œä¿æŒæ•¸å­¸å…¬å¼æ ¼å¼
    result = result.replace('\n\n', '\n').strip()
    # æ¨™æº–åŒ–ç©ºç™½å­—ç¬¦
    import re
    result = re.sub(r' +', ' ', result)  # å¤šå€‹ç©ºæ ¼è®Šæˆä¸€å€‹
    result = re.sub(r' *\n+ *', '\n', result)  # æ¸…ç†æ›è¡Œå‘¨åœçš„ç©ºæ ¼
    
    return result

def test_html_extraction():
    """æ¸¬è©¦HTMLæå–åŠŸèƒ½"""
    test_url = "https://arxiv.org/html/2507.21856"
    test_dir = "./test_html_extract"
    
    print(f"ğŸ§ª Testing HTML extraction with: {test_url}")
    try:
        extract_from_html(test_url, test_dir)
        
        # æª¢æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        text_file = os.path.join(test_dir, "text.md")
        if os.path.exists(text_file):
            with open(text_file, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"âœ… Markdownæ–‡ä»¶ç”ŸæˆæˆåŠŸ")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
            print(f"ğŸ“ è¡Œæ•¸: {len(content.split(chr(10)))}")
            
            # é¡¯ç¤ºå‰å¹¾è¡Œé è¦½
            lines = content.split('\n')[:10]
            print("ğŸ“– å…§å®¹é è¦½:")
            for line in lines:
                if line.strip():
                    print(f"  {line}")
        else:
            print("âŒ Markdownæ–‡ä»¶æœªç”Ÿæˆ")
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

# ====== æ¸¬è©¦æ–¹å¼ ======
def simple_test():
    """ç°¡å–®æ¸¬è©¦å‡½æ•¸"""
    test_url = "https://arxiv.org/html/2507.21856"
    test_dir = "./test_h2_p_extract"
    
    print(f"ğŸ§ª æ¸¬è©¦ h2 æ¨™é¡Œ + p æ®µè½æå–æ¨¡å¼")
    print(f"URL: {test_url}")
    print(f"è¼¸å‡ºç›®éŒ„: {test_dir}")
    
    try:
        extract_from_html(test_url, test_dir)
        
        # æª¢æŸ¥çµæœ
        text_file = os.path.join(test_dir, "text.md")
        if os.path.exists(text_file):
            with open(text_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"âœ… æå–å®Œæˆ!")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
            
            # çµ±è¨ˆç« ç¯€æ•¸é‡
            h2_count = content.count('## ')
            paragraph_count = len([line for line in content.split('\n') if line.strip() and not line.startswith('#')])
            
            print(f"ğŸ“š æ‰¾åˆ° {h2_count} å€‹ h2 ç« ç¯€æ¨™é¡Œ")
            print(f"ğŸ“ æ‰¾åˆ° {paragraph_count} å€‹æ®µè½")
            
            # é¡¯ç¤ºå‰å¹¾è¡Œ
            lines = content.split('\n')[:20]
            print("ğŸ“– å…§å®¹é è¦½:")
            for line in lines:
                if line.strip():
                    print(f"  {line}")
                    
        else:
            print("âŒ æ–‡ä»¶æœªç”Ÿæˆ")
            
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

# extract_from_html("https://arxiv.org/html/2507.21856", "./2507.21856")
# test_html_extraction()  # åŸ·è¡Œæ¸¬è©¦
# simple_test()  # åŸ·è¡Œæ–°çš„ç°¡å–®æ¸¬è©¦
